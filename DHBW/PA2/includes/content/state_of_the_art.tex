\chapter{State of the Art}
\label{State_of_the_Art}

While this project assumes that the reader is already familiar with the basic concepts of High Performance Computation
as well as having a basic understanding of the Cloud Computing technology stack, especially containerization and software defined infrastructure,
this chapter will give a brief overview of the current state of the art in the field of High Performance Computing and Cloud Computing.

\section{Containerization}
Containerization, a concept originated within the cloud computing community, refers to the process of
isolating a process from the rest of the operating system's user space. It
provides a virtual environment for the process to run in, which can be
specifically tailored to the process's needs without impacting the rest of the
system. Such isolation is particularly advantageous when running multiple
processes on the same machine, as it eliminates the risk of interference between
processes or with the system itself\footcite{shenoiHPCEducationDomain2019}. A
notable benefit of containerization is its streamlined dependency management,
enabling different versions of the same library to coexist on a single machine,
with each process accessing the version it requires. Containerization offers a
lightweight alternative to traditional virtual machines by building on the
host's kernel to isolate only the user space, rather than emulating an entire
operating system\footcite{WhatContainerDocker}. Although prevalent in cloud
computing, containerization technologies are only recently being adopted in
\ac{HPC} domains. The \ac{HPC} community's cautious approach
towards containerization stems from the imperative to preserve system
performance, which is paramount in high-stakes computational tasks. This section
will delve into the nuances of containerization within the \ac{HPC} ecosystem,
elucidating the reasons behind its gradual integration and the potential
benefits it heralds for future applications.
\subsection*{Container Solutions}

Currently the containerization paradigm is spreading rapidly with multiple solutions available, the most popular being Docker \footcite{StackOverflowDeveloper}.
However, within the \ac{HPC} community, Singularity \footcite{IntroductionSoftwareContainers} has gained more and more traction, while the previous options have historically been more popular in \ac{CC}
The main differentiating factor between the two is that Docker is reliant on the Docker daemon, which needs to be run as root, while Singularity does not need a daemon and can be run as unprivileged user.
This makes Singularity generally more appealing to \ac{HPC} users, as it is more in line with the security policies of most \ac{HPC} clusters, and simplifies the process of running containers on \ac{HPC} clusters.
But Docker is still the more popular option in \ac{CC}, as it is more flexible and has a larger ecosystem of tools and services built around it which makes rapid prototyping and development easier.

These tools make use of two features of the Linux kernel, namely cgroups and namespaces \footcite{WhatContainerDocker}.
Cgroups are used to limit the resources a process can use, while namespaces are used to isolate the user space of the process.

\subsection*{Software defined Infrastructure}

Through the advent of large scale cloud endeavors and their offering of dynamic scaling services, the need for a way to automatically manage and partition the underlying infrastructure arose.
This led to the development of software defined infrastructure, which is the process of abstracting the underlying infrastructure and managing it through software.
The paradigm of software defined X is especially pronounced in the field of networking\footcite{xiaSurveySoftwareDefinedNetworking2015}.
Software defined networking is used to manage the underlying network infrastructure,
where the different underlying network devices are abstracted to a homogeneous network, which then gets managed through virtualization and software\footcite{baurCloudOrchestrationFeatures2015}.
Large scale cloud providers like Amazon Web Services, Microsoft Azure and Google Cloud Platform all make use of software defined infrastructure to manage their underlying infrastructure.

\subsection*{Large Scale Container Orchestration}

These large scale software defined infrastructures are used in tandem and managed by large scale orchestrators like Openshift\footcite{RedHatOpenShift}, Openstack \footcite{OpenSourceCloud} and Kubernetes\footcite{ProductionGradeContainerOrchestration}. 
The orchestrators interface with the underlying infrastructure and manage the services running on top of it and provide a unified interface for the user to interact with.
This makes it possible to largely run on any, even heterogeneous, infrastructure, while providing a highly flexible and scalable interface for the user to interact with.
Make it especially useful for large scale cloud providers, as it allows them to provide a unified interface for their users, while still being able to use different underlying infrastructure.

\section{High Performance Computing Frameworks}

\ac{HPC} is the process of using multiple computing nodes in collaboration to solve a problem in less time or with better accuracy than would be possible with a single node.
 The computational power provided by \ac{HPC} is used in a wide variety of fields, ranging from weather forecasting to computational fluid dynamics and even in the field of machine learning.
While \ac{HPC} is as diverse as the problems it is used to solve, there are some common patterns that can be found in most \ac{HPC} problems.
Generally \ac{HPC} problems can be divided into two categories, \ac{LCP} and \ac{TCP} problems, which are discussed in more detail in \ref{state_of_the_art_tcp}.


\subsection{Loosely Coupled Problems}

\ac{LCP} also known in the industry as "embarrassingly parallel" \footcite{brownEngineeringBeowulfstyleCompute2004} problems are problems that can be broken up into small independent tasks that can be executed in parallel.
Problems like a monte carlo simulation or a matrix multiplication are good examples of \ac{LCP} problems.

From the perspective of the user, \ac{LCP} problems are the easiest to solve by using \ac{HPC}, as they can be solved by simply running the same program multiple times with different input parameters across multiple nodes.
Seen from the perspective of the \ac{HPC} system, \ac{LCP} problems trivial at best as they require very little communication between the different nodes.

The \ac{CC} community has already seen the need in this aspect and has developed and integrated systems to solve these,
such as containerized versions of MapReduce \footcite{camacho-rodriguezApacheHiveMapReduce2019} or cloud native solutions like Pachyderm \footcite{HomePage2022}


\subsection{Tightly Coupled Problems}
\label{state_of_the_art_tcp}
In contrast to the highly paralelizable  \ac{LCP} problems, \ac{TCP} problems are problems that can not be broken up into smaller independent tasks that can be executed in parallel,
instead of working independently, each atomic task needs to communicate at least with one other task.
A good example of a \ac{TCP} problem are the n-body problems, where the position of each body is dependent on the position of all other bodies.

These kind of problems require a more sufisticated approach, as they need to share state across the multiple computing nodes.
Sharing the necessary information between the nodes while maintaining performance at scale, has become its own sector in industry and science.

Communication protocols like \ac{MPI}\footcite{snirMPICompleteReference1998},\break \ac{OpenSHMEM} \footcite{chapmanIntroducingOpenSHMEMSHMEM2010}
have been developed to facilitate the communication between nodes and processes, but are still being continuously developed and optimized.