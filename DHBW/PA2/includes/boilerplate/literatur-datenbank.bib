@inproceedings{beltreEnablingHPCWorkloads2019,
  title = {Enabling {{HPC Workloads}} on {{Cloud Infrastructure Using Kubernetes Container Orchestration Mechanisms}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  author = {Beltre, Angel M. and Saha, Pankaj and Govindaraju, Madhusudhan and Younge, Andrew and Grant, Ryan E.},
  date = {2019-11},
  pages = {11--20},
  doi = {10.1109/CANOPIE-HPC49598.2019.00007},
  abstract = {Containers offer a broad array of benefits, including a consistent lightweight runtime environment through OS-level virtualization, as well as low overhead to maintain and scale applications with high efficiency. Moreover, containers are known to package and deploy applications consistently across varying infrastructures. Container orchestrators manage a large number of containers for microservices based cloud applications. However, the use of such service orchestration frameworks towards HPC workloads remains relatively unexplored. In this paper we study the potential use of Kubernetes on HPC infrastructure for use by the scientific community. We directly compare both its features and performance against Docker Swarm and bare metal execution of HPC applications. Herein, we detail the configurations required for Kubernetes to operate with containerized MPI applications, specifically accounting for operations such as (1) underlying device access, (2) inter-container communication across different hosts, and (3) configuration limitations. This evaluation quantifies the performance difference between representative MPI workloads running both on bare metal and containerized orchestration frameworks with Kubernetes, operating over both Ethernet and InfiniBand interconnects. Our results show that Kubernetes and Docker Swarm can achieve near bare metal performance over RDMA communication when high performance transports are enabled. Our results also show that Kubernetes presents overheads for several HPC applications over TCP/IP protocol. However, Docker Swarm's throughput is near bare metal performance for the same applications.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  keywords = {Cloud computing,Cloud Computing,Container,Containers,HPC,Kubernetes,Metals,Overlay networks,Performance,Protocols,Runtime,Software},
  file = {/home/jon/Zotero/storage/H4CQDCX7/Beltre et al. - 2019 - Enabling HPC Workloads on Cloud Infrastructure Usi.pdf;/home/jon/Zotero/storage/7VFPBGTM/stamp.html}
}

@article{dubeFutureHPCInternet2021,
  title = {Future of {{HPC}}: {{The Internet}} of {{Workflows}}},
  shorttitle = {Future of {{HPC}}},
  author = {Dube, Nicolas and Roweth, Duncan and Faraboschi, Paolo and Milojicic, Dejan},
  date = {2021-09},
  journaltitle = {IEEE Internet Computing},
  volume = {25},
  number = {5},
  pages = {26--34},
  issn = {1941-0131},
  doi = {10.1109/MIC.2021.3103236},
  abstract = {Driven by convergence with artificial intelligence and data analytics, increased heterogeneity, and a hybrid cloud/on-premise delivery model, dynamic composition of workflows will be a key design criteria of future high-performance computing (HPC) systems. While tightly coupled HPC workloads will continue to execute on dedicated supercomputers, other jobs will run elsewhere, including public clouds, and at the edge. Connecting these distributed computing tasks into coherent applications that can perform at scale is what we call “Internet of Workflows.”},
  eventtitle = {{{IEEE Internet Computing}}},
  keywords = {Cloud computing,Computational modeling,Convergence,Data analysis,Data models,High performance computing,Software,Supercomputers},
  file = {/home/jon/Zotero/storage/YWK8E86B/Dube et al. - 2021 - Future of HPC The Internet of Workflows.pdf;/home/jon/Zotero/storage/5VV4WTBU/stamp.html}
}

@online{FabricAttachedMemory,
  title = {Fabric {{Attached Memory}} – {{Hardware}} and {{Software Architecture}} | {{SNIA}}},
  url = {https://www.snia.org/educational-library/fabric-attached-memory-%E2%80%93-hardware-and-software-architecture-2023},
  urldate = {2023-06-29},
  file = {/home/jon/Zotero/storage/7KSBFQH6/fabric-attached-memory-–-hardware-and-software-architecture-2023.html}
}

@online{FichtnerArbeitsmaterial,
  title = {Fichtner\_{{OR}}: {{Arbeitsmaterial}}},
  url = {https://elearning.dhbw-stuttgart.de/moodle/course/resources.php?id=5585},
  urldate = {2023-07-03},
  file = {/home/jon/Zotero/storage/K3N8SYUK/resources.html}
}

@incollection{hainesWorkflowOrchestrationApache2022,
  title = {Workflow {{Orchestration}} with {{Apache Airflow}}},
  booktitle = {Modern {{Data Engineering}} with {{Apache Spark}}: {{A Hands-On Guide}} for {{Building Mission-Critical Streaming Applications}}},
  author = {Haines, Scott},
  date = {2022},
  pages = {255--295},
  publisher = {{Springer}},
  file = {/home/jon/Zotero/storage/SJJULDJR/978-1-4842-7452-1_8.html}
}

@book{harenslakDataPipelinesApache2021,
  title = {Data {{Pipelines}} with {{Apache Airflow}}},
  author = {Harenslak, Bas P. and family=Ruiter, given=Julian, prefix=de, useprefix=true},
  date = {2021},
  publisher = {{Simon and Schuster}},
  file = {/home/jon/Zotero/storage/Y3K839KY/books.html}
}

@report{kennyKubernetesHPCAdministration2021,
  title = {Kubernetes for {{HPC Administration}}.},
  author = {Kenny, Joseph and Knight, Samuel},
  date = {2021-09-01},
  number = {SAND2021-11507C},
  institution = {{Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Sandia National Laboratories, SNL California}},
  url = {https://www.osti.gov/biblio/1887730},
  urldate = {2023-06-29},
  abstract = {Abstract not provided.},
  langid = {english},
  file = {/home/jon/Zotero/storage/H8BAICLF/Kenny and Knight - 2021 - Kubernetes for HPC Administration..pdf}
}

@inproceedings{mitchellExplorationWorkflowManagement2019,
  title = {Exploration of {{Workflow Management Systems Emerging Features}} from {{Users Perspectives}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Mitchell, Ryan and Pottier, Loїc and Jacobs, Steve and family=Silva, given=Rafael Ferreira, prefix=da, useprefix=false and Rynge, Mats and Vahi, Karan and Deelman, Ewa},
  date = {2019-12},
  pages = {4537--4544},
  doi = {10.1109/BigData47090.2019.9005494},
  abstract = {There has been a recent emergence of new workflow applications focused on data analytics and machine learning. This emergence has precipitated a change in the workflow management landscape, causing the development of new dataoriented workflow management systems (WMSs) in addition to the earlier standard of task-oriented WMSs. In this paper, we summarize three general workflow use-cases and explore the unique requirements of each use-case in order to understand how WMSs from both workflow management models meet the requirements of each workflow use-case from the user’s perspective. We analyze the applicability of the two models by carefully describing each model and by providing an examination of the different variations of WMSs that fall under the task driven model. To illustrate the strengths and weaknesses of each workflow management model, we summarize the key features of four production-ready WMSs: Pegasus, Makeflow, Apache Airflow, and Pachyderm. To deepen our analysis of the four WMSs examined in this paper,we implement three real-world use-cases to highlight the specifications and features of each WMS. We present our final assessment of each WMS after considering the following factors: usability, performance, ease of deployment, and relevance. The purpose of this work is to offer insights from the user’s perspective into the research challenges that WMSs currently face due to the evolving workflow landscape.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Atmospheric modeling,Big Data,Biological system modeling,Computational modeling,Data analysis,Data-driven.,Machine learning,Scientific workflow,Task analysis,Task-driven,Workflow Management System},
  file = {/home/jon/Zotero/storage/SEQSYHKQ/Mitchell et al. - 2019 - Exploration of Workflow Management Systems Emergin.pdf;/home/jon/Zotero/storage/Y3WVEQNM/9005494.html}
}

@article{novellaContainerbasedBioinformaticsPachyderm2019,
  title = {Container-Based Bioinformatics with {{Pachyderm}}},
  author = {Novella, Jon Ander and Emami Khoonsari, Payam and Herman, Stephanie and Whitenack, Daniel and Capuccini, Marco and Burman, Joachim and Kultima, Kim and Spjuth, Ola},
  editor = {Wren, Jonathan},
  date = {2019-03-01},
  journaltitle = {Bioinformatics},
  volume = {35},
  number = {5},
  pages = {839--846},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/bty699},
  url = {https://academic.oup.com/bioinformatics/article/35/5/839/5068160},
  urldate = {2023-06-29},
  abstract = {Motivation: Computational biologists face many challenges related to data size, and they need to manage complicated analyses often including multiple stages and multiple tools, all of which must be deployed to modern infrastructures. To address these challenges and maintain reproducibility of results, researchers need (i) a reliable way to run processing stages in any computational environment, (ii) a well-defined way to orchestrate those processing stages and (iii) a data management layer that tracks data as it moves through the processing pipeline.},
  langid = {english},
  file = {/home/jon/Zotero/storage/C5LF9253/Novella et al. - 2019 - Container-based bioinformatics with Pachyderm.pdf}
}

@article{schmidtEvaluationDataScience,
  title = {Evaluation von Data Science Workflow Engines für Kubernetes},
  author = {Schmidt, David},
  langid = {ngerman},
  file = {/home/jon/Zotero/storage/NJMEJZQJ/Schmidt - Evaluation von Data Science Workflow Engines für K.pdf}
}

@article{wildeMethodenspektrumWirtschaftsinformatikUeberblick,
  title = {Methodenspektrum der Wirtschaftsinformatik: Überblick und Portfoliobildung},
  author = {Wilde, Thomas and Hess, Thomas},
  langid = {ngerman},
  file = {/home/jon/Zotero/storage/89XM8EK6/Wilde and Hess - Methodenspektrum der Wirtschaftsinformatik Überbl.pdf}
}

@inproceedings{yooSLURMSimpleLinux2003,
  title = {{{SLURM}}: {{Simple Linux Utility}} for {{Resource Management}}},
  shorttitle = {{{SLURM}}},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {44--60},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/10968987_3},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
  isbn = {978-3-540-39727-4},
  langid = {english},
  keywords = {Exit Status,Lawrence Livermore National Laboratory,Message Authentication Code,Remote Execution,Resource Management System},
  file = {/home/jon/Zotero/storage/CIP9VDIE/Yoo et al. - 2003 - SLURM Simple Linux Utility for Resource Managemen.pdf}
}

@article{zhouContainerOrchestrationHPC2021,
  title = {Container Orchestration on {{HPC}} Systems through {{Kubernetes}}},
  author = {Zhou, Naweiluo and Georgiou, Yiannis and Pospieszny, Marcin and Zhong, Li and Zhou, Huan and Niethammer, Christoph and Pejak, Branislav and Marko, Oskar and Hoppe, Dennis},
  date = {2021-02-22},
  journaltitle = {Journal of Cloud Computing},
  shortjournal = {Journal of Cloud Computing},
  volume = {10},
  number = {1},
  pages = {16},
  issn = {2192-113X},
  doi = {10.1186/s13677-021-00231-z},
  url = {https://doi.org/10.1186/s13677-021-00231-z},
  urldate = {2023-07-02},
  abstract = {Containerisation demonstrates its efficiency in application deployment in Cloud Computing. Containers can encapsulate complex programs with their dependencies in isolated environments making applications more portable, hence are being adopted in High Performance Computing (HPC) clusters. Singularity, initially designed for HPC systems, has become their de facto standard container runtime. Nevertheless, conventional HPC workload managers lack micro-service support and deeply-integrated container management, as opposed to container orchestrators. We introduce a Torque-Operator which serves as a bridge between HPC workload manager (TORQUE) and container orchestrator (Kubernetes). We propose a hybrid architecture that integrates HPC and Cloud clusters seamlessly with little interference to HPC systems where container orchestration is performed on two levels.},
  keywords = {Cloud computing,Container orchestration,HPC workload manager,Kubernetes,Singularity,TORQUE},
  file = {/home/jon/Zotero/storage/UMHY2J4N/Zhou et al. - 2021 - Container orchestration on HPC systems through Kub.pdf;/home/jon/Zotero/storage/EYUHHD39/s13677-021-00231-z.html}
}
