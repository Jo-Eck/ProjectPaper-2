@online{AirflowVsLuigi,
  title = {Airflow vs {{Luigi}} vs {{Argo}} vs {{Kubeflow}} vs {{MLFlow}}},
  url = {https://www.datarevenue.com/en-blog/airflow-vs-luigi-vs-argo-vs-mlflow-vs-kubeflow},
  urldate = {2023-10-04},
  abstract = {Luigi is simple, Airflow is powerful, and Argo is Kubernetes-based. This post offers a detailed description and comparison of workflow orchestration tools.},
  langid = {english},
  file = {/home/jon/Zotero/storage/XSJ3JUAS/airflow-vs-luigi-vs-argo-vs-mlflow-vs-kubeflow.html}
}

@inproceedings{beltreEnablingHPCWorkloads2019,
  title = {Enabling {{HPC Workloads}} on {{Cloud Infrastructure Using Kubernetes Container Orchestration Mechanisms}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  author = {Beltre, Angel M. and Saha, Pankaj and Govindaraju, Madhusudhan and Younge, Andrew and Grant, Ryan E.},
  date = {2019-11},
  pages = {11--20},
  doi = {10.1109/CANOPIE-HPC49598.2019.00007},
  abstract = {Containers offer a broad array of benefits, including a consistent lightweight runtime environment through OS-level virtualization, as well as low overhead to maintain and scale applications with high efficiency. Moreover, containers are known to package and deploy applications consistently across varying infrastructures. Container orchestrators manage a large number of containers for microservices based cloud applications. However, the use of such service orchestration frameworks towards HPC workloads remains relatively unexplored. In this paper we study the potential use of Kubernetes on HPC infrastructure for use by the scientific community. We directly compare both its features and performance against Docker Swarm and bare metal execution of HPC applications. Herein, we detail the configurations required for Kubernetes to operate with containerized MPI applications, specifically accounting for operations such as (1) underlying device access, (2) inter-container communication across different hosts, and (3) configuration limitations. This evaluation quantifies the performance difference between representative MPI workloads running both on bare metal and containerized orchestration frameworks with Kubernetes, operating over both Ethernet and InfiniBand interconnects. Our results show that Kubernetes and Docker Swarm can achieve near bare metal performance over RDMA communication when high performance transports are enabled. Our results also show that Kubernetes presents overheads for several HPC applications over TCP/IP protocol. However, Docker Swarm's throughput is near bare metal performance for the same applications.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  keywords = {Cloud computing,Cloud Computing,Container,Containers,HPC,Kubernetes,Metals,Overlay networks,Performance,Protocols,Runtime,Software},
  file = {/home/jon/Zotero/storage/H4CQDCX7/Beltre et al. - 2019 - Enabling HPC Workloads on Cloud Infrastructure Usi.pdf;/home/jon/Zotero/storage/7VFPBGTM/stamp.html}
}

@article{buddeWhatPrototyping1992,
  title = {What Is Prototyping?},
  author = {Budde, Reinhard and Kautz, Karlheinz and Kuhlenkamp, Karin and Züllighoven, Heinz},
  date = {1992-01-01},
  journaltitle = {Information Technology \& People},
  volume = {6},
  number = {2/3},
  pages = {89--95},
  publisher = {{MCB UP Ltd}},
  issn = {0959-3845},
  doi = {10.1108/EUM0000000003546},
  url = {https://doi.org/10.1108/EUM0000000003546},
  urldate = {2023-10-02},
  abstract = {Explains and defines prototyping in terms of its character, actors and types. Examines four aspects: its use in the software development process, its goals, horizontal and vertical and the relationship between prototype and application system. Clarifies the distinction between breadboard and prototype.},
  keywords = {Management information systems,Prototyping},
  file = {/home/jon/Zotero/storage/AYPIBGZ5/Budde et al. - 1992 - What is prototyping.pdf}
}

@inproceedings{bzeznikNixHPCPackage2017,
  title = {Nix as {{HPC}} Package Management System},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{HPC User Support Tools}}},
  author = {Bzeznik, Bruno and Henriot, Oliver and Reis, Valentin and Richard, Olivier and Tavard, Laure},
  date = {2017-11-12},
  series = {{{HUST}}'17},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3152493.3152556},
  url = {https://dl.acm.org/doi/10.1145/3152493.3152556},
  urldate = {2023-10-03},
  abstract = {Modern High Performance Computing systems are becoming larger and more heterogeneous. The proper management of software for the users of such systems poses a significant challenge. These users run very diverse applications that may be compiled with proprietary tools for specialized hardware. Moreover, the application life-cycle of these software may exceed the lifetime of the HPC systems themselves. These difficulties motivate the use of specialized package management systems. In this paper, we outline an approach to HPC package development, deployment, management, sharing, and reuse based on the Nix functional package manager. We report our experience with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble over a 12 month period and compare it to other existing approaches.},
  isbn = {978-1-4503-5130-0},
  keywords = {High Performance Computing,Nix,Package Management System},
  file = {/home/jon/Zotero/storage/24FY2CY2/Bzeznik et al. - 2017 - Nix as HPC package management system.pdf}
}

@inproceedings{canonCasePortabilityReproducibility2019,
  title = {A {{Case}} for {{Portability}} and {{Reproducibility}} of {{HPC Containers}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  author = {Canon, Richard S. and Younge, Andrew},
  date = {2019-11},
  pages = {49--54},
  publisher = {{IEEE}},
  location = {{Denver, CO, USA}},
  doi = {10.1109/CANOPIE-HPC49598.2019.00012},
  url = {https://ieeexplore.ieee.org/document/8950982/},
  urldate = {2023-10-04},
  abstract = {Containerized computing is quickly changing the landscape for the development and deployment of many HPC applications. Containers are able to lower the barrier of entry for emerging workloads to leverage supercomputing resources. However, containers are no silver bullet for deploying HPC software and there are several challenges ahead in which the community must address to ensure container workloads can be reproducible and inter-operable.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  isbn = {978-1-72816-028-3},
  langid = {english},
  file = {/home/jon/Zotero/storage/4U4A7U8C/Canon and Younge - 2019 - A Case for Portability and Reproducibility of HPC .pdf}
}

@inproceedings{canonCasePortabilityReproducibility2019a,
  title = {A {{Case}} for {{Portability}} and {{Reproducibility}} of {{HPC Containers}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  author = {Canon, Richard S. and Younge, Andrew},
  date = {2019-11},
  pages = {49--54},
  doi = {10.1109/CANOPIE-HPC49598.2019.00012},
  url = {https://ieeexplore.ieee.org/abstract/document/8950982},
  urldate = {2023-10-04},
  abstract = {Containerized computing is quickly changing the landscape for the development and deployment of many HPC applications. Containers are able to lower the barrier of entry for emerging workloads to leverage supercomputing resources. However, containers are no silver bullet for deploying HPC software and there are several challenges ahead in which the community must address to ensure container workloads can be reproducible and inter-operable. In this paper, we discuss several challenges in utilizing containers for HPC applications and the current approaches used in many HPC container runtimes. These approaches have been proven to enable high-performance execution of containers at scale with the appropriate runtimes. However, the use of these techniques are still ad hoc, test the limits of container workload portability, and several gaps likely remain. We discuss those remaining gaps and propose several potential solutions, including custom container label tagging and runtime hooks as a first step in managing HPC system library complexity.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  file = {/home/jon/Zotero/storage/F2GCRHJ8/Canon and Younge - 2019 - A Case for Portability and Reproducibility of HPC .pdf;/home/jon/Zotero/storage/GKLSSHWY/8950982.html}
}

@video{computerphileApacheSparkComputerphile2018,
  entrysubtype = {video},
  title = {Apache {{Spark}} - {{Computerphile}}},
  editor = {{Computerphile}},
  editortype = {director},
  date = {2018-12-12},
  url = {https://www.youtube.com/watch?v=tDVPcqGpEnM},
  urldate = {2023-10-02},
  abstract = {Analysing big data stored on a cluster is not easy. Spark allows you to do so much more than just MapReduce. Rebecca Tickle takes us through some code.  https://www.facebook.com/computerphile https://twitter.com/computer\_phile This video was filmed and edited by Sean Riley. Computer Science at the University of Nottingham: https://bit.ly/nottscomputer Computerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com}
}

@video{computerphileMapReduceComputerphile2018,
  entrysubtype = {video},
  title = {{{MapReduce}} - {{Computerphile}}},
  editor = {{Computerphile}},
  editortype = {director},
  date = {2018-04-12},
  url = {https://www.youtube.com/watch?v=cvhKoniK5Uo},
  urldate = {2023-10-02},
  abstract = {Peforming operations in parallel on big data. Rebecca Tickle explains MapReduce.  https://www.facebook.com/computerphile https://twitter.com/computer\_phile This video was filmed and edited by Sean Riley. Computer Science at the University of Nottingham: https://bit.ly/nottscomputer Computerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com}
}

@article{dubeFutureHPCInternet2021,
  title = {Future of {{HPC}}: {{The Internet}} of {{Workflows}}},
  shorttitle = {Future of {{HPC}}},
  author = {Dube, Nicolas and Roweth, Duncan and Faraboschi, Paolo and Milojicic, Dejan},
  date = {2021-09},
  journaltitle = {IEEE Internet Computing},
  volume = {25},
  number = {5},
  pages = {26--34},
  issn = {1941-0131},
  doi = {10.1109/MIC.2021.3103236},
  abstract = {Driven by convergence with artificial intelligence and data analytics, increased heterogeneity, and a hybrid cloud/on-premise delivery model, dynamic composition of workflows will be a key design criteria of future high-performance computing (HPC) systems. While tightly coupled HPC workloads will continue to execute on dedicated supercomputers, other jobs will run elsewhere, including public clouds, and at the edge. Connecting these distributed computing tasks into coherent applications that can perform at scale is what we call “Internet of Workflows.”},
  eventtitle = {{{IEEE Internet Computing}}},
  keywords = {Cloud computing,Computational modeling,Convergence,Data analysis,Data models,High performance computing,Software,Supercomputers},
  file = {/home/jon/Zotero/storage/YWK8E86B/Dube et al. - 2021 - Future of HPC The Internet of Workflows.pdf;/home/jon/Zotero/storage/5VV4WTBU/stamp.html}
}

@article{duboisWhyJohnnyCan2003,
  title = {Why {{Johnny}} Can't Build [Portable Scientific Software]},
  author = {Dubois, P.F. and Epperly, T. and Kumfert, G.},
  date = {2003-09},
  journaltitle = {Computing in Science \& Engineering},
  volume = {5},
  number = {5},
  pages = {83--88},
  issn = {1558-366X},
  doi = {10.1109/MCISE.2003.1225867},
  url = {https://ieeexplore.ieee.org/abstract/document/1225867},
  urldate = {2023-10-03},
  abstract = {The title of this article refers to Rudolph Flesch's famous 1955 book, "Why Johnny Can't Read", which called attention to a nationwide decline in reading ability. Here, the author wants to talk about another situation in which an important ability is lacking: the ability to create significant, portable scientific software. The author discusses some of the reasons this problem exists and suggests some approaches to solving it that seem promising.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  file = {/home/jon/Zotero/storage/ZW2VL9QH/Dubois et al. - 2003 - Why Johnny can't build [portable scientific softwa.pdf}
}

@article{eckerthHASPHPCApplications,
  title = {{{HASP}}: An {{HPC Applications Services Platform}} for {{Data Analysts}}},
  author = {Eckerth, Jon and Kuno, Harumi and Singhal, Sharad and Byrne, John and Dwaraki, Abhishek and Serebryakov, Sergey},
  abstract = {High Performance Computing (HPC) frameworks make it possible to write large scale, complex, interwoven parallel programs involving intensive inter-process communication. Pachyderm is a very effective tool that simplifies the process of implementing scalable, reproducible and resilient data pipelines. To data analysts, the ability to operate and interactively analyze and reproduce hyper-scale data pipelines is invaluable. To this end, we are building a proof-of-concept programming framework that empowers a HPC framework with reproducible and resilient data pipelines. This allows a catalog of datasets and functions that operate on them to be exposed, enabling data analysts to create, publish, share, and reuse workflows and new datasets seamlessly with low overhead.},
  langid = {english},
  file = {/home/jon/Zotero/storage/SU4LPITE/Eckerth et al. - HASP an HPC Applications Services Platform for Da.pdf}
}

@article{egwutuohaSurveyFaultTolerance2013,
  title = {A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart Implementations for High Performance Computing Systems},
  author = {Egwutuoha, Ifeanyi P. and Levy, David and Selic, Bran and Chen, Shiping},
  date = {2013-09-01},
  journaltitle = {J Supercomput},
  volume = {65},
  number = {3},
  pages = {1302--1326},
  issn = {1573-0484},
  doi = {10.1007/s11227-013-0884-0},
  url = {https://doi.org/10.1007/s11227-013-0884-0},
  urldate = {2023-10-03},
  abstract = {In recent years, High Performance Computing (HPC) systems have been shifting from expensive massively parallel architectures to clusters of commodity PCs to take advantage of cost and performance benefits. Fault tolerance in such systems is a growing concern for long-running applications. In this paper, we briefly review the failure rates of HPC systems and also survey the fault tolerance approaches for HPC systems and issues with these approaches. Rollback-recovery techniques which are most often used for long-running applications on HPC clusters are discussed because they are widely used for long-running applications on HPC systems. Specifically, the feature requirements of rollback-recovery are discussed and a taxonomy is developed for over twenty popular checkpoint/restart solutions. The intent of this paper is to aid researchers in the domain as well as to facilitate development of new checkpointing solutions.},
  langid = {english},
  keywords = {Checkpoint/restart,Clusters,Fault tolerance,High Performance Computing (HPC),Performance,Reliability},
  file = {/home/jon/Zotero/storage/Y6WW7RNX/Egwutuoha et al. - 2013 - A survey of fault tolerance mechanisms and checkpo.pdf}
}

@online{FabricAttachedMemory,
  title = {Fabric {{Attached Memory}} – {{Hardware}} and {{Software Architecture}} | {{SNIA}}},
  url = {https://www.snia.org/educational-library/fabric-attached-memory-%E2%80%93-hardware-and-software-architecture-2023},
  urldate = {2023-06-29},
  file = {/home/jon/Zotero/storage/7KSBFQH6/fabric-attached-memory-–-hardware-and-software-architecture-2023.html}
}

@online{FichtnerArbeitsmaterial,
  title = {Fichtner\_{{OR}}: {{Arbeitsmaterial}}},
  url = {https://elearning.dhbw-stuttgart.de/moodle/course/resources.php?id=5585},
  urldate = {2023-07-03},
  file = {/home/jon/Zotero/storage/K3N8SYUK/resources.html}
}

@inproceedings{gamblinSpackPackageManager2015,
  title = {The {{Spack}} Package Manager: Bringing Order to {{HPC}} Software Chaos},
  shorttitle = {The {{Spack}} Package Manager},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and family=Supinski, given=Bronis R., prefix=de, useprefix=true and Futral, Scott},
  date = {2015-11-15},
  series = {{{SC}} '15},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2807591.2807623},
  url = {https://dl.acm.org/doi/10.1145/2807591.2807623},
  urldate = {2023-10-03},
  abstract = {Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.},
  isbn = {978-1-4503-3723-6},
  file = {/home/jon/Zotero/storage/F2XDEKSE/Gamblin et al. - 2015 - The Spack package manager bringing order to HPC s.pdf}
}

@incollection{hainesWorkflowOrchestrationApache2022,
  title = {Workflow {{Orchestration}} with {{Apache Airflow}}},
  booktitle = {Modern {{Data Engineering}} with {{Apache Spark}}: {{A Hands-On Guide}} for {{Building Mission-Critical Streaming Applications}}},
  author = {Haines, Scott},
  date = {2022},
  pages = {255--295},
  publisher = {{Springer}},
  file = {/home/jon/Zotero/storage/SJJULDJR/978-1-4842-7452-1_8.html}
}

@book{harenslakDataPipelinesApache2021,
  title = {Data {{Pipelines}} with {{Apache Airflow}}},
  author = {Harenslak, Bas P. and family=Ruiter, given=Julian, prefix=de, useprefix=true},
  date = {2021},
  publisher = {{Simon and Schuster}},
  file = {/home/jon/Zotero/storage/Y3K839KY/books.html}
}

@inproceedings{higginsOrchestratingDockerContainers2015,
  title = {Orchestrating {{Docker Containers}} in the {{HPC Environment}}},
  booktitle = {High {{Performance Computing}}},
  author = {Higgins, Joshua and Holmes, Violeta and Venters, Colin},
  editor = {Kunkel, Julian M. and Ludwig, Thomas},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {506--513},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-20119-1_36},
  abstract = {Linux container technology has more than proved itself useful in cloud computing as a lightweight alternative to virtualisation, whilst still offering good enough resource isolation. Docker is emerging as a popular runtime for managing Linux containers, providing both management tools and a simple file format. Research into the performance of containers compared to traditional Virtual Machines and bare metal shows that containers can achieve near native speeds in processing, memory and network throughput. A technology born in the cloud, it is making inroads into scientific computing both as a format for sharing experimental applications and as a paradigm for cloud based execution. However, it has unexplored uses in traditional cluster and grid computing. It provides a run time environment in which there is an opportunity for typical cluster and parallel applications to execute at native speeds, whilst being bundled with their own specific (or legacy) library versions and support software. This offers a solution to the Achilles heel of cluster and grid computing that requires the user to hold intimate knowledge of the local software infrastructure. Using Docker brings us a step closer to more effective job and resource management within the cluster by providing both a common definition format and a repeatable execution environment. In this paper we present the results of our work in deploying Docker containers in the cluster environment and an evaluation of its suitability as a runtime for high performance parallel execution. Our findings suggest that containers can be used to tailor the run time environment for an MPI application without compromising performance, and would provide better Quality of Service for users of scientific computing.},
  isbn = {978-3-319-20119-1},
  langid = {english},
  keywords = {Cluster,Docker,Grids,Linux containers,Run time environment}
}

@inproceedings{higginsOrchestratingDockerContainers2015a,
  title = {Orchestrating {{Docker Containers}} in the {{HPC Environment}}},
  booktitle = {High {{Performance Computing}}},
  author = {Higgins, Joshua and Holmes, Violeta and Venters, Colin},
  editor = {Kunkel, Julian M. and Ludwig, Thomas},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {506--513},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-20119-1_36},
  abstract = {Linux container technology has more than proved itself useful in cloud computing as a lightweight alternative to virtualisation, whilst still offering good enough resource isolation. Docker is emerging as a popular runtime for managing Linux containers, providing both management tools and a simple file format. Research into the performance of containers compared to traditional Virtual Machines and bare metal shows that containers can achieve near native speeds in processing, memory and network throughput. A technology born in the cloud, it is making inroads into scientific computing both as a format for sharing experimental applications and as a paradigm for cloud based execution. However, it has unexplored uses in traditional cluster and grid computing. It provides a run time environment in which there is an opportunity for typical cluster and parallel applications to execute at native speeds, whilst being bundled with their own specific (or legacy) library versions and support software. This offers a solution to the Achilles heel of cluster and grid computing that requires the user to hold intimate knowledge of the local software infrastructure. Using Docker brings us a step closer to more effective job and resource management within the cluster by providing both a common definition format and a repeatable execution environment. In this paper we present the results of our work in deploying Docker containers in the cluster environment and an evaluation of its suitability as a runtime for high performance parallel execution. Our findings suggest that containers can be used to tailor the run time environment for an MPI application without compromising performance, and would provide better Quality of Service for users of scientific computing.},
  isbn = {978-3-319-20119-1},
  langid = {english},
  keywords = {Cluster,Docker,Grids,Linux containers,Run time environment}
}

@online{HomePage2022,
  title = {Home {{Page}}},
  date = {2022-07-06T22:56:08+00:00},
  url = {https://www.pachyderm.com/},
  urldate = {2023-10-04},
  abstract = {Data-driven pipelines automatically trigger based on detecting data changes.},
  langid = {american},
  organization = {{Pachyderm}},
  file = {/home/jon/Zotero/storage/5TYVL4H6/www.pachyderm.com.html}
}

@inproceedings{hosteEasyBuildBuildingSoftware2012,
  title = {{{EasyBuild}}: {{Building Software}} with {{Ease}}},
  shorttitle = {{{EasyBuild}}},
  booktitle = {2012 {{SC Companion}}: {{High Performance Computing}}, {{Networking Storage}} and {{Analysis}}},
  author = {Hoste, Kenneth and Timmerman, Jens and Georges, Andy and De Weirdt, Stijn},
  date = {2012-11},
  pages = {572--582},
  doi = {10.1109/SC.Companion.2012.81},
  url = {https://ieeexplore.ieee.org/abstract/document/6495863},
  urldate = {2023-10-03},
  abstract = {Maintaining a collection of software installations for a diverse user base can be a tedious, repetitive, error-prone and time-consuming task. Because most end-user software packages for an HPC environment are not readily available in existing OS package managers, they require significant extra effort from the user support team. Reducing this effort would free up a large amount of time for tackling more urgent tasks. In this work, we present EasyBuild, a software installation framework written in Python that aims to support the various installation procedures used by the vast collection of software packages that are typically installed in an HPC environment - catering to widely different user profiles. It is built on top of existing tools, and provides support for well-established installation procedures. Supporting customised installation procedures requires little effort, and sharing implementations of installation procedures becomes very easy. Installing software packages that are supported can be done by issuing a single command, even if dependencies are not available yet. Hence, it simplifies the task of HPC site support teams, and even allows end-users to keep their software installations consistent and up to date.},
  eventtitle = {2012 {{SC Companion}}: {{High Performance Computing}}, {{Networking Storage}} and {{Analysis}}},
  file = {/home/jon/Zotero/storage/KYS8DCP6/Hoste et al. - 2012 - EasyBuild Building Software with Ease.pdf;/home/jon/Zotero/storage/YPGHB5BZ/6495863.html}
}

@report{kennyKubernetesHPCAdministration2021,
  title = {Kubernetes for {{HPC Administration}}.},
  author = {Kenny, Joseph and Knight, Samuel},
  date = {2021-09-01},
  number = {SAND2021-11507C},
  institution = {{Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Sandia National Laboratories, SNL California}},
  url = {https://www.osti.gov/biblio/1887730},
  urldate = {2023-06-29},
  abstract = {Abstract not provided.},
  langid = {english},
  file = {/home/jon/Zotero/storage/H8BAICLF/Kenny and Knight - 2021 - Kubernetes for HPC Administration..pdf}
}

@article{kreuzbergerMachineLearningOperations2023,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle = {Machine {{Learning Operations}} ({{MLOps}})},
  author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {31866--31879},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3262138},
  url = {https://ieeexplore.ieee.org/document/10081336/},
  urldate = {2023-10-04},
  abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we contribute to the body of knowledge by providing an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we provide a comprehensive definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
  langid = {english},
  file = {/home/jon/Zotero/storage/MG5ZD2ZZ/Kreuzberger et al. - 2023 - Machine Learning Operations (MLOps) Overview, Def.pdf}
}

@online{MachineLearningOperations,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10081336},
  urldate = {2023-10-04},
  file = {/home/jon/Zotero/storage/XPQQV6Y4/10081336.html}
}

@inproceedings{merrillArkoudaInteractiveData2019,
  title = {Arkouda: Interactive Data Exploration Backed by {{Chapel}}},
  shorttitle = {Arkouda},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 6th on {{Chapel Implementers}} and {{Users Workshop}}},
  author = {Merrill, Michael and Reus, William and Neumann, Timothy},
  date = {2019-06-22},
  series = {{{CHIUW}} 2019},
  pages = {28},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3329722.3330148},
  url = {https://dl.acm.org/doi/10.1145/3329722.3330148},
  urldate = {2023-10-04},
  abstract = {Exploratory data analysis (EDA) is the prerequisite for all data science. EDA is non-negotiably interactive—by far the most popular environment for EDA is a Jupyter notebook—and, as datasets grow, increasingly computationally intensive. Several existing projects attempt to combine interactivity and distributed computation using programming paradigms and tools from cloud computing, but none of these projects have come close to meeting our needs for high-performance EDA. To fill this gap, we have developed a prototype, called arkouda, that allows a user to interactively issue massively parallel computations on distributed data.},
  isbn = {978-1-4503-6800-1},
  keywords = {Chapel,Exploratory data analysis (EDA),Jupyter,NumPy,Python},
  file = {/home/jon/Zotero/storage/QRW9S4AT/Merrill et al. - 2019 - Arkouda interactive data exploration backed by Ch.pdf}
}

@inproceedings{mitchellExplorationWorkflowManagement2019,
  title = {Exploration of {{Workflow Management Systems Emerging Features}} from {{Users Perspectives}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Mitchell, Ryan and Pottier, Loїc and Jacobs, Steve and family=Silva, given=Rafael Ferreira, prefix=da, useprefix=false and Rynge, Mats and Vahi, Karan and Deelman, Ewa},
  date = {2019-12},
  pages = {4537--4544},
  doi = {10.1109/BigData47090.2019.9005494},
  abstract = {There has been a recent emergence of new workflow applications focused on data analytics and machine learning. This emergence has precipitated a change in the workflow management landscape, causing the development of new dataoriented workflow management systems (WMSs) in addition to the earlier standard of task-oriented WMSs. In this paper, we summarize three general workflow use-cases and explore the unique requirements of each use-case in order to understand how WMSs from both workflow management models meet the requirements of each workflow use-case from the user’s perspective. We analyze the applicability of the two models by carefully describing each model and by providing an examination of the different variations of WMSs that fall under the task driven model. To illustrate the strengths and weaknesses of each workflow management model, we summarize the key features of four production-ready WMSs: Pegasus, Makeflow, Apache Airflow, and Pachyderm. To deepen our analysis of the four WMSs examined in this paper,we implement three real-world use-cases to highlight the specifications and features of each WMS. We present our final assessment of each WMS after considering the following factors: usability, performance, ease of deployment, and relevance. The purpose of this work is to offer insights from the user’s perspective into the research challenges that WMSs currently face due to the evolving workflow landscape.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Atmospheric modeling,Big Data,Biological system modeling,Computational modeling,Data analysis,Data-driven.,Machine learning,Scientific workflow,Task analysis,Task-driven,Workflow Management System},
  file = {/home/jon/Zotero/storage/SEQSYHKQ/Mitchell et al. - 2019 - Exploration of Workflow Management Systems Emergin.pdf;/home/jon/Zotero/storage/Y3WVEQNM/9005494.html}
}

@article{nikolovConceptualizationScalableExecution2021,
  title = {Conceptualization and Scalable Execution of Big Data Workflows Using Domain-Specific Languages and Software Containers},
  author = {Nikolov, Nikolay and Dessalk, Yared Dejene and Khan, Akif Quddus and Soylu, Ahmet and Matskin, Mihhail and Payberah, Amir H. and Roman, Dumitru},
  date = {2021-12},
  journaltitle = {Internet of Things},
  volume = {16},
  pages = {100440},
  issn = {25426605},
  doi = {10.1016/j.iot.2021.100440},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2542660521000834},
  urldate = {2023-10-04},
  abstract = {Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.},
  langid = {english},
  file = {/home/jon/Zotero/storage/5N3N98KA/Nikolov et al. - 2021 - Conceptualization and scalable execution of big da.pdf}
}

@article{nikolovConceptualizationScalableExecution2021a,
  title = {Conceptualization and Scalable Execution of Big Data Workflows Using Domain-Specific Languages and Software Containers},
  author = {Nikolov, Nikolay and Dessalk, Yared Dejene and Khan, Akif Quddus and Soylu, Ahmet and Matskin, Mihhail and Payberah, Amir H. and Roman, Dumitru},
  date = {2021-12-01},
  journaltitle = {Internet of Things},
  volume = {16},
  pages = {100440},
  issn = {2542-6605},
  doi = {10.1016/j.iot.2021.100440},
  url = {https://www.sciencedirect.com/science/article/pii/S2542660521000834},
  urldate = {2023-10-04},
  abstract = {Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.},
  keywords = {Big data workflows,Domain-specific languages,Internet of Things,Software containers},
  file = {/home/jon/Zotero/storage/V245RD2W/Nikolov et al. - 2021 - Conceptualization and scalable execution of big da.pdf;/home/jon/Zotero/storage/SC7YEDJX/S2542660521000834.html}
}

@article{novellaContainerbasedBioinformaticsPachyderm2019,
  title = {Container-Based Bioinformatics with {{Pachyderm}}},
  author = {Novella, Jon Ander and Emami Khoonsari, Payam and Herman, Stephanie and Whitenack, Daniel and Capuccini, Marco and Burman, Joachim and Kultima, Kim and Spjuth, Ola},
  editor = {Wren, Jonathan},
  date = {2019-03-01},
  journaltitle = {Bioinformatics},
  volume = {35},
  number = {5},
  pages = {839--846},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/bty699},
  url = {https://academic.oup.com/bioinformatics/article/35/5/839/5068160},
  urldate = {2023-06-29},
  abstract = {Motivation: Computational biologists face many challenges related to data size, and they need to manage complicated analyses often including multiple stages and multiple tools, all of which must be deployed to modern infrastructures. To address these challenges and maintain reproducibility of results, researchers need (i) a reliable way to run processing stages in any computational environment, (ii) a well-defined way to orchestrate those processing stages and (iii) a data management layer that tracks data as it moves through the processing pipeline.},
  langid = {english},
  file = {/home/jon/Zotero/storage/C5LF9253/Novella et al. - 2019 - Container-based bioinformatics with Pachyderm.pdf}
}

@online{pachydermPachyderm,
  type = {Code repository},
  title = {Pachyderm},
  author = {{pachyderm}},
  url = {https://github.com/pachyderm/pachyderm},
  urldate = {2023-10-04},
  organization = {{Github repository}},
  file = {/home/jon/Zotero/storage/W96EDFF5/pachyderm.html}
}

@article{rufDemystifyingMLOpsPresenting2021,
  title = {Demystifying {{MLOps}} and {{Presenting}} a {{Recipe}} for the {{Selection}} of {{Open-Source Tools}}},
  author = {Ruf, Philipp and Madan, Manav and Reich, Christoph and Ould-Abdeslam, Djaffar},
  date = {2021-01},
  journaltitle = {Applied Sciences},
  volume = {11},
  number = {19},
  pages = {8861},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app11198861},
  url = {https://www.mdpi.com/2076-3417/11/19/8861},
  urldate = {2023-10-04},
  abstract = {Nowadays, machine learning projects have become more and more relevant to various real-world use cases. The success of complex Neural Network models depends upon many factors, as the requirement for structured and machine learning-centric project development management arises. Due to the multitude of tools available for different operational phases, responsibilities and requirements become more and more unclear. In this work, Machine Learning Operations (MLOps) technologies and tools for every part of the overall project pipeline, as well as involved roles, are examined and clearly defined. With the focus on the inter-connectivity of specific tools and comparison by well-selected requirements of MLOps, model performance, input data, and system quality metrics are briefly discussed. By identifying aspects of machine learning, which can be reused from project to project, open-source tools which help in specific parts of the pipeline, and possible combinations, an overview of support in MLOps is given. Deep learning has revolutionized the field of Image processing, and building an automated machine learning workflow for object detection is of great interest for many organizations. For this, a simple MLOps workflow for object detection with images is portrayed.},
  issue = {19},
  langid = {english},
  keywords = {MlOps,quality metrics,tool comparison,workflow automation},
  file = {/home/jon/Zotero/storage/4BS96SWB/Ruf et al. - 2021 - Demystifying MLOps and Presenting a Recipe for the.pdf}
}

@article{sayersCLoudApplicationServices2015,
  title = {{{CLoud Application Services Platform}}},
  author = {Sayers, Craig and Laffitte, Hernan and Reddy, Prakash and Ozonat, Kivanc and Sayal, Mehmet and Simitsis, Alkis and Singhal, Sharad and Koutrika, Georgia and Das, Mahashweta and Aji, Ablimit and Bosamiya, Hitesh Amrutlal and Riss, Marcelo and Wilkinson, Kevin},
  date = {2015},
  langid = {english},
  file = {/home/jon/Zotero/storage/PN7ADPIX/Sayers et al. - 2015 - CLoud Application Services Platform.pdf}
}

@article{sayersCloudApplicationServices2015,
  title = {Cloud {{Application Services Platform}} ({{CLASP}}): {{User}} Guide, Introduction, and Operation},
  shorttitle = {Cloud {{Application Services Platform}} ({{CLASP}})},
  author = {Sayers, C. and Laffitte, H. and Reddy, P. and Ozonat, K. and Sayal, M. and Simitsis, A. and Singhal, Sharad and Koutrika, Georgia and Das, M. and Aji, A. and Bosamiya, H.A. and Riss, Marcelo and Wilkinson, Kevin and Lucio, J.C.O. and Cantal, A.G.S. and Carvalho, C.R.M.},
  date = {2015-06-11},
  pages = {1--60},
  abstract = {Software developers at large tech companies spend a lot of time writing code for tasks that colleagues elsewhere in the organization have already addressed. Scripts are rarely written or documented with discovery in mind, and the APIs on which they depend are frequently inconsistent, further limiting reuse. For mobile devices the App Catalog serves as an essential intermediary, streamlining the process both for developers and end users. We've created an experimental platform called CLASP (Cloud Application Services Platform) applying that model by publishing services and datasets instead of apps. It includes support for existing APIs, and we've also created an SDK (software development kit), so our users can write other operations themselves and easily publish in the catalog for later discovery and reuse. CLASP allows us to take a very diverse set of operations and make them all available through a consistent compositional interface. For example, you can retrieve log messages using OneView and analyze the text using Autonomy, or gather system configuration using iLO interfaces and persist the results in a Vertica Database. Our internal deployment now has more than 2,000 services, and has been used by more than 150 developers. It allows application developers to discover, test, and use services while providing a seamless app-catalog-type experience for service developers, allowing them to code and test locally while semi-automating the process of publishing those in the catalog. Developed by HP Labs, CLASP is an experimental platform and not a product. This report includes an introduction, quick-start guide, and implementational details.}
}

@article{schmidtEvaluationDataScience,
  title = {Evaluation von Data Science Workflow Engines für Kubernetes},
  author = {Schmidt, David},
  langid = {ngerman},
  file = {/home/jon/Zotero/storage/NJMEJZQJ/Schmidt - Evaluation von Data Science Workflow Engines für K.pdf}
}

@article{wildeMethodenspektrumWirtschaftsinformatikUeberblick,
  title = {Methodenspektrum der Wirtschaftsinformatik: Überblick und Portfoliobildung},
  author = {Wilde, Thomas and Hess, Thomas},
  langid = {ngerman},
  file = {/home/jon/Zotero/storage/89XM8EK6/Wilde and Hess - Methodenspektrum der Wirtschaftsinformatik Überbl.pdf}
}

@article{wrattenReproducibleScalableShareable2021,
  title = {Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers},
  author = {Wratten, Laura and Wilm, Andreas and Göke, Jonathan},
  date = {2021-10},
  journaltitle = {Nat Methods},
  volume = {18},
  number = {10},
  pages = {1161--1168},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01254-9},
  url = {https://www.nature.com/articles/s41592-021-01254-9},
  urldate = {2023-10-04},
  abstract = {The rapid growth of high-throughput technologies has transformed biomedical research. With the increasing amount and complexity of data, scalability and reproducibility have become essential not just for experiments, but also for computational analysis. However, transforming data into information involves running a large number of tools, optimizing parameters, and integrating dynamically changing reference data. Workflow managers were developed in response to such challenges. They simplify pipeline development, optimize resource usage, handle software installation and versions, and run on different compute platforms, enabling workflow portability and sharing. In this Perspective, we highlight key features of workflow managers, compare commonly used approaches for bioinformatics workflows, and provide a guide for computational and noncomputational users. We outline community-curated pipeline initiatives that enable novice and experienced users to perform complex, best-practice analyses without having to manually assemble workflows. In sum, we illustrate how workflow managers contribute to making computational analysis in biomedical research shareable, scalable, and reproducible.},
  issue = {10},
  langid = {english},
  keywords = {Computational platforms and environments,Programming language,Software}
}

@inproceedings{yooSLURMSimpleLinux2003,
  title = {{{SLURM}}: {{Simple Linux Utility}} for {{Resource Management}}},
  shorttitle = {{{SLURM}}},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {44--60},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/10968987_3},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
  isbn = {978-3-540-39727-4},
  langid = {english},
  keywords = {Exit Status,Lawrence Livermore National Laboratory,Message Authentication Code,Remote Execution,Resource Management System},
  file = {/home/jon/Zotero/storage/CIP9VDIE/Yoo et al. - 2003 - SLURM Simple Linux Utility for Resource Managemen.pdf}
}

@article{youScalingSupportVector2015,
  title = {Scaling {{Support Vector Machines}} on Modern {{HPC}} Platforms},
  author = {You, Yang and Fu, Haohuan and Song, Shuaiwen Leon and Randles, Amanda and Kerbyson, Darren and Marquez, Andres and Yang, Guangwen and Hoisie, Adolfy},
  date = {2015-02-01},
  journaltitle = {Journal of Parallel and Distributed Computing},
  series = {Special {{Issue}} on {{Architecture}} and {{Algorithms}} for {{Irregular Applications}}},
  volume = {76},
  pages = {16--31},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.09.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0743731514001683},
  urldate = {2023-10-03},
  abstract = {Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for~x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4–84×~and 18–47×~speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x~GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns.},
  keywords = {Dynamic modeling,Machine learning models,Multi- \& many-core architectures,Optimization techniques,Performance analysis,Support Vector Machine},
  file = {/home/jon/Zotero/storage/VY6R76FR/You et al. - 2015 - Scaling Support Vector Machines on modern HPC plat.pdf;/home/jon/Zotero/storage/J3798C3J/S0743731514001683.html}
}

@article{zhouContainerOrchestrationHPC2021,
  title = {Container Orchestration on {{HPC}} Systems through {{Kubernetes}}},
  author = {Zhou, Naweiluo and Georgiou, Yiannis and Pospieszny, Marcin and Zhong, Li and Zhou, Huan and Niethammer, Christoph and Pejak, Branislav and Marko, Oskar and Hoppe, Dennis},
  date = {2021-02-22},
  journaltitle = {Journal of Cloud Computing},
  volume = {10},
  number = {1},
  pages = {16},
  issn = {2192-113X},
  doi = {10.1186/s13677-021-00231-z},
  url = {https://doi.org/10.1186/s13677-021-00231-z},
  urldate = {2023-07-02},
  abstract = {Containerisation demonstrates its efficiency in application deployment in Cloud Computing. Containers can encapsulate complex programs with their dependencies in isolated environments making applications more portable, hence are being adopted in High Performance Computing (HPC) clusters. Singularity, initially designed for HPC systems, has become their de facto standard container runtime. Nevertheless, conventional HPC workload managers lack micro-service support and deeply-integrated container management, as opposed to container orchestrators. We introduce a Torque-Operator which serves as a bridge between HPC workload manager (TORQUE) and container orchestrator (Kubernetes). We propose a hybrid architecture that integrates HPC and Cloud clusters seamlessly with little interference to HPC systems where container orchestration is performed on two levels.},
  keywords = {Cloud computing,Container orchestration,HPC workload manager,Kubernetes,Singularity,TORQUE},
  file = {/home/jon/Zotero/storage/UMHY2J4N/Zhou et al. - 2021 - Container orchestration on HPC systems through Kub.pdf;/home/jon/Zotero/storage/EYUHHD39/s13677-021-00231-z.html}
}

@online{zotero-41,
  pubstate = {preprint}
}
