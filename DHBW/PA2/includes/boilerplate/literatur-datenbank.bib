@inproceedings{beltreEnablingHPCWorkloads2019,
  title = {Enabling {{HPC Workloads}} on {{Cloud Infrastructure Using Kubernetes Container Orchestration Mechanisms}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  author = {Beltre, Angel M. and Saha, Pankaj and Govindaraju, Madhusudhan and Younge, Andrew and Grant, Ryan E.},
  date = {2019-11},
  pages = {11--20},
  doi = {10.1109/CANOPIE-HPC49598.2019.00007},
  abstract = {Containers offer a broad array of benefits, including a consistent lightweight runtime environment through OS-level virtualization, as well as low overhead to maintain and scale applications with high efficiency. Moreover, containers are known to package and deploy applications consistently across varying infrastructures. Container orchestrators manage a large number of containers for microservices based cloud applications. However, the use of such service orchestration frameworks towards HPC workloads remains relatively unexplored. In this paper we study the potential use of Kubernetes on HPC infrastructure for use by the scientific community. We directly compare both its features and performance against Docker Swarm and bare metal execution of HPC applications. Herein, we detail the configurations required for Kubernetes to operate with containerized MPI applications, specifically accounting for operations such as (1) underlying device access, (2) inter-container communication across different hosts, and (3) configuration limitations. This evaluation quantifies the performance difference between representative MPI workloads running both on bare metal and containerized orchestration frameworks with Kubernetes, operating over both Ethernet and InfiniBand interconnects. Our results show that Kubernetes and Docker Swarm can achieve near bare metal performance over RDMA communication when high performance transports are enabled. Our results also show that Kubernetes presents overheads for several HPC applications over TCP/IP protocol. However, Docker Swarm's throughput is near bare metal performance for the same applications.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Workshop}} on {{Containers}} and {{New Orchestration Paradigms}} for {{Isolated Environments}} in {{HPC}} ({{CANOPIE-HPC}})},
  keywords = {Cloud computing,Cloud Computing,Container,Containers,HPC,Kubernetes,Metals,Overlay networks,Performance,Protocols,Runtime,Software},
  file = {/home/jon/Zotero/storage/H4CQDCX7/Beltre et al. - 2019 - Enabling HPC Workloads on Cloud Infrastructure Usi.pdf;/home/jon/Zotero/storage/7VFPBGTM/stamp.html}
}

@article{buddeWhatPrototyping1992,
  title = {What Is Prototyping?},
  author = {Budde, Reinhard and Kautz, Karlheinz and Kuhlenkamp, Karin and Züllighoven, Heinz},
  date = {1992-01-01},
  journaltitle = {Information Technology \& People},
  volume = {6},
  number = {2/3},
  pages = {89--95},
  publisher = {{MCB UP Ltd}},
  issn = {0959-3845},
  doi = {10.1108/EUM0000000003546},
  url = {https://doi.org/10.1108/EUM0000000003546},
  urldate = {2023-10-02},
  abstract = {Explains and defines prototyping in terms of its character, actors and types. Examines four aspects: its use in the software development process, its goals, horizontal and vertical and the relationship between prototype and application system. Clarifies the distinction between breadboard and prototype.},
  keywords = {Management information systems,Prototyping},
  file = {/home/jon/Zotero/storage/AYPIBGZ5/Budde et al. - 1992 - What is prototyping.pdf}
}

@inproceedings{bzeznikNixHPCPackage2017,
  title = {Nix as {{HPC}} Package Management System},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{HPC User Support Tools}}},
  author = {Bzeznik, Bruno and Henriot, Oliver and Reis, Valentin and Richard, Olivier and Tavard, Laure},
  date = {2017-11-12},
  series = {{{HUST}}'17},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3152493.3152556},
  url = {https://dl.acm.org/doi/10.1145/3152493.3152556},
  urldate = {2023-10-03},
  abstract = {Modern High Performance Computing systems are becoming larger and more heterogeneous. The proper management of software for the users of such systems poses a significant challenge. These users run very diverse applications that may be compiled with proprietary tools for specialized hardware. Moreover, the application life-cycle of these software may exceed the lifetime of the HPC systems themselves. These difficulties motivate the use of specialized package management systems. In this paper, we outline an approach to HPC package development, deployment, management, sharing, and reuse based on the Nix functional package manager. We report our experience with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble over a 12 month period and compare it to other existing approaches.},
  isbn = {978-1-4503-5130-0},
  keywords = {High Performance Computing,Nix,Package Management System},
  file = {/home/jon/Zotero/storage/24FY2CY2/Bzeznik et al. - 2017 - Nix as HPC package management system.pdf}
}

@video{computerphileApacheSparkComputerphile2018,
  entrysubtype = {video},
  title = {Apache {{Spark}} - {{Computerphile}}},
  editor = {{Computerphile}},
  editortype = {director},
  date = {2018-12-12},
  url = {https://www.youtube.com/watch?v=tDVPcqGpEnM},
  urldate = {2023-10-02},
  abstract = {Analysing big data stored on a cluster is not easy. Spark allows you to do so much more than just MapReduce. Rebecca Tickle takes us through some code.  https://www.facebook.com/computerphile https://twitter.com/computer\_phile This video was filmed and edited by Sean Riley. Computer Science at the University of Nottingham: https://bit.ly/nottscomputer Computerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com}
}

@video{computerphileMapReduceComputerphile2018,
  entrysubtype = {video},
  title = {{{MapReduce}} - {{Computerphile}}},
  editor = {{Computerphile}},
  editortype = {director},
  date = {2018-04-12},
  url = {https://www.youtube.com/watch?v=cvhKoniK5Uo},
  urldate = {2023-10-02},
  abstract = {Peforming operations in parallel on big data. Rebecca Tickle explains MapReduce.  https://www.facebook.com/computerphile https://twitter.com/computer\_phile This video was filmed and edited by Sean Riley. Computer Science at the University of Nottingham: https://bit.ly/nottscomputer Computerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com}
}

@article{dubeFutureHPCInternet2021,
  title = {Future of {{HPC}}: {{The Internet}} of {{Workflows}}},
  shorttitle = {Future of {{HPC}}},
  author = {Dube, Nicolas and Roweth, Duncan and Faraboschi, Paolo and Milojicic, Dejan},
  date = {2021-09},
  journaltitle = {IEEE Internet Computing},
  volume = {25},
  number = {5},
  pages = {26--34},
  issn = {1941-0131},
  doi = {10.1109/MIC.2021.3103236},
  abstract = {Driven by convergence with artificial intelligence and data analytics, increased heterogeneity, and a hybrid cloud/on-premise delivery model, dynamic composition of workflows will be a key design criteria of future high-performance computing (HPC) systems. While tightly coupled HPC workloads will continue to execute on dedicated supercomputers, other jobs will run elsewhere, including public clouds, and at the edge. Connecting these distributed computing tasks into coherent applications that can perform at scale is what we call “Internet of Workflows.”},
  eventtitle = {{{IEEE Internet Computing}}},
  keywords = {Cloud computing,Computational modeling,Convergence,Data analysis,Data models,High performance computing,Software,Supercomputers},
  file = {/home/jon/Zotero/storage/YWK8E86B/Dube et al. - 2021 - Future of HPC The Internet of Workflows.pdf;/home/jon/Zotero/storage/5VV4WTBU/stamp.html}
}

@article{duboisWhyJohnnyCan2003,
  title = {Why {{Johnny}} Can't Build [Portable Scientific Software]},
  author = {Dubois, P.F. and Epperly, T. and Kumfert, G.},
  date = {2003-09},
  journaltitle = {Computing in Science \& Engineering},
  volume = {5},
  number = {5},
  pages = {83--88},
  issn = {1558-366X},
  doi = {10.1109/MCISE.2003.1225867},
  url = {https://ieeexplore.ieee.org/abstract/document/1225867},
  urldate = {2023-10-03},
  abstract = {The title of this article refers to Rudolph Flesch's famous 1955 book, "Why Johnny Can't Read", which called attention to a nationwide decline in reading ability. Here, the author wants to talk about another situation in which an important ability is lacking: the ability to create significant, portable scientific software. The author discusses some of the reasons this problem exists and suggests some approaches to solving it that seem promising.},
  eventtitle = {Computing in {{Science}} \& {{Engineering}}},
  file = {/home/jon/Zotero/storage/ZW2VL9QH/Dubois et al. - 2003 - Why Johnny can't build [portable scientific softwa.pdf}
}

@article{eckerthHASPHPCApplications,
  title = {{{HASP}}: An {{HPC Applications Services Platform}} for {{Data Analysts}}},
  author = {Eckerth, Jon and Kuno, Harumi and Singhal, Sharad and Byrne, John and Dwaraki, Abhishek and Serebryakov, Sergey},
  abstract = {High Performance Computing (HPC) frameworks make it possible to write large scale, complex, interwoven parallel programs involving intensive inter-process communication. Pachyderm is a very effective tool that simplifies the process of implementing scalable, reproducible and resilient data pipelines. To data analysts, the ability to operate and interactively analyze and reproduce hyper-scale data pipelines is invaluable. To this end, we are building a proof-of-concept programming framework that empowers a HPC framework with reproducible and resilient data pipelines. This allows a catalog of datasets and functions that operate on them to be exposed, enabling data analysts to create, publish, share, and reuse workflows and new datasets seamlessly with low overhead.},
  langid = {english},
  file = {/home/jon/Zotero/storage/SU4LPITE/Eckerth et al. - HASP an HPC Applications Services Platform for Da.pdf}
}

@article{egwutuohaSurveyFaultTolerance2013,
  title = {A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart Implementations for High Performance Computing Systems},
  author = {Egwutuoha, Ifeanyi P. and Levy, David and Selic, Bran and Chen, Shiping},
  date = {2013-09-01},
  journaltitle = {J Supercomput},
  volume = {65},
  number = {3},
  pages = {1302--1326},
  issn = {1573-0484},
  doi = {10.1007/s11227-013-0884-0},
  url = {https://doi.org/10.1007/s11227-013-0884-0},
  urldate = {2023-10-03},
  abstract = {In recent years, High Performance Computing (HPC) systems have been shifting from expensive massively parallel architectures to clusters of commodity PCs to take advantage of cost and performance benefits. Fault tolerance in such systems is a growing concern for long-running applications. In this paper, we briefly review the failure rates of HPC systems and also survey the fault tolerance approaches for HPC systems and issues with these approaches. Rollback-recovery techniques which are most often used for long-running applications on HPC clusters are discussed because they are widely used for long-running applications on HPC systems. Specifically, the feature requirements of rollback-recovery are discussed and a taxonomy is developed for over twenty popular checkpoint/restart solutions. The intent of this paper is to aid researchers in the domain as well as to facilitate development of new checkpointing solutions.},
  langid = {english},
  keywords = {Checkpoint/restart,Clusters,Fault tolerance,High Performance Computing (HPC),Performance,Reliability},
  file = {/home/jon/Zotero/storage/Y6WW7RNX/Egwutuoha et al. - 2013 - A survey of fault tolerance mechanisms and checkpo.pdf}
}

@online{FabricAttachedMemory,
  title = {Fabric {{Attached Memory}} – {{Hardware}} and {{Software Architecture}} | {{SNIA}}},
  url = {https://www.snia.org/educational-library/fabric-attached-memory-%E2%80%93-hardware-and-software-architecture-2023},
  urldate = {2023-06-29},
  file = {/home/jon/Zotero/storage/7KSBFQH6/fabric-attached-memory-–-hardware-and-software-architecture-2023.html}
}

@online{FichtnerArbeitsmaterial,
  title = {Fichtner\_{{OR}}: {{Arbeitsmaterial}}},
  url = {https://elearning.dhbw-stuttgart.de/moodle/course/resources.php?id=5585},
  urldate = {2023-07-03},
  file = {/home/jon/Zotero/storage/K3N8SYUK/resources.html}
}

@inproceedings{gamblinSpackPackageManager2015,
  title = {The {{Spack}} Package Manager: Bringing Order to {{HPC}} Software Chaos},
  shorttitle = {The {{Spack}} Package Manager},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee, Gregory L. and Moody, Adam and family=Supinski, given=Bronis R., prefix=de, useprefix=true and Futral, Scott},
  date = {2015-11-15},
  series = {{{SC}} '15},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2807591.2807623},
  url = {https://dl.acm.org/doi/10.1145/2807591.2807623},
  urldate = {2023-10-03},
  abstract = {Large HPC centers spend considerable time supporting software for thousands of users, but the complexity of HPC software is quickly outpacing the capabilities of existing software management tools. Scientific applications require specific versions of compilers, MPI, and other dependency libraries, so using a single, standard software stack is infeasible. However, managing many configurations is difficult because the configuration space is combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore National Laboratory to manage this complexity. Spack provides a novel, recursive specification syntax to invoke parametric builds of packages and dependencies. It allows any number of builds to coexist on the same system, and it ensures that installed packages can find their dependencies, regardless of the environment. We show through real-world use cases that Spack supports diverse and demanding applications, bringing order to HPC software chaos.},
  isbn = {978-1-4503-3723-6},
  file = {/home/jon/Zotero/storage/F2XDEKSE/Gamblin et al. - 2015 - The Spack package manager bringing order to HPC s.pdf}
}

@incollection{hainesWorkflowOrchestrationApache2022,
  title = {Workflow {{Orchestration}} with {{Apache Airflow}}},
  booktitle = {Modern {{Data Engineering}} with {{Apache Spark}}: {{A Hands-On Guide}} for {{Building Mission-Critical Streaming Applications}}},
  author = {Haines, Scott},
  date = {2022},
  pages = {255--295},
  publisher = {{Springer}},
  file = {/home/jon/Zotero/storage/SJJULDJR/978-1-4842-7452-1_8.html}
}

@book{harenslakDataPipelinesApache2021,
  title = {Data {{Pipelines}} with {{Apache Airflow}}},
  author = {Harenslak, Bas P. and family=Ruiter, given=Julian, prefix=de, useprefix=true},
  date = {2021},
  publisher = {{Simon and Schuster}},
  file = {/home/jon/Zotero/storage/Y3K839KY/books.html}
}

@inproceedings{higginsOrchestratingDockerContainers2015,
  title = {Orchestrating {{Docker Containers}} in the {{HPC Environment}}},
  booktitle = {High {{Performance Computing}}},
  author = {Higgins, Joshua and Holmes, Violeta and Venters, Colin},
  editor = {Kunkel, Julian M. and Ludwig, Thomas},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {506--513},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-20119-1_36},
  abstract = {Linux container technology has more than proved itself useful in cloud computing as a lightweight alternative to virtualisation, whilst still offering good enough resource isolation. Docker is emerging as a popular runtime for managing Linux containers, providing both management tools and a simple file format. Research into the performance of containers compared to traditional Virtual Machines and bare metal shows that containers can achieve near native speeds in processing, memory and network throughput. A technology born in the cloud, it is making inroads into scientific computing both as a format for sharing experimental applications and as a paradigm for cloud based execution. However, it has unexplored uses in traditional cluster and grid computing. It provides a run time environment in which there is an opportunity for typical cluster and parallel applications to execute at native speeds, whilst being bundled with their own specific (or legacy) library versions and support software. This offers a solution to the Achilles heel of cluster and grid computing that requires the user to hold intimate knowledge of the local software infrastructure. Using Docker brings us a step closer to more effective job and resource management within the cluster by providing both a common definition format and a repeatable execution environment. In this paper we present the results of our work in deploying Docker containers in the cluster environment and an evaluation of its suitability as a runtime for high performance parallel execution. Our findings suggest that containers can be used to tailor the run time environment for an MPI application without compromising performance, and would provide better Quality of Service for users of scientific computing.},
  isbn = {978-3-319-20119-1},
  langid = {english},
  keywords = {Cluster,Docker,Grids,Linux containers,Run time environment}
}

@inproceedings{higginsOrchestratingDockerContainers2015a,
  title = {Orchestrating {{Docker Containers}} in the {{HPC Environment}}},
  booktitle = {High {{Performance Computing}}},
  author = {Higgins, Joshua and Holmes, Violeta and Venters, Colin},
  editor = {Kunkel, Julian M. and Ludwig, Thomas},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {506--513},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-20119-1_36},
  abstract = {Linux container technology has more than proved itself useful in cloud computing as a lightweight alternative to virtualisation, whilst still offering good enough resource isolation. Docker is emerging as a popular runtime for managing Linux containers, providing both management tools and a simple file format. Research into the performance of containers compared to traditional Virtual Machines and bare metal shows that containers can achieve near native speeds in processing, memory and network throughput. A technology born in the cloud, it is making inroads into scientific computing both as a format for sharing experimental applications and as a paradigm for cloud based execution. However, it has unexplored uses in traditional cluster and grid computing. It provides a run time environment in which there is an opportunity for typical cluster and parallel applications to execute at native speeds, whilst being bundled with their own specific (or legacy) library versions and support software. This offers a solution to the Achilles heel of cluster and grid computing that requires the user to hold intimate knowledge of the local software infrastructure. Using Docker brings us a step closer to more effective job and resource management within the cluster by providing both a common definition format and a repeatable execution environment. In this paper we present the results of our work in deploying Docker containers in the cluster environment and an evaluation of its suitability as a runtime for high performance parallel execution. Our findings suggest that containers can be used to tailor the run time environment for an MPI application without compromising performance, and would provide better Quality of Service for users of scientific computing.},
  isbn = {978-3-319-20119-1},
  langid = {english},
  keywords = {Cluster,Docker,Grids,Linux containers,Run time environment}
}

@online{HomePage2022,
  title = {Home {{Page}}},
  date = {2022-07-06T22:56:08+00:00},
  url = {https://www.pachyderm.com/},
  urldate = {2023-10-04},
  abstract = {Data-driven pipelines automatically trigger based on detecting data changes.},
  langid = {american},
  organization = {{Pachyderm}},
  file = {/home/jon/Zotero/storage/5TYVL4H6/www.pachyderm.com.html}
}

@inproceedings{hosteEasyBuildBuildingSoftware2012,
  title = {{{EasyBuild}}: {{Building Software}} with {{Ease}}},
  shorttitle = {{{EasyBuild}}},
  booktitle = {2012 {{SC Companion}}: {{High Performance Computing}}, {{Networking Storage}} and {{Analysis}}},
  author = {Hoste, Kenneth and Timmerman, Jens and Georges, Andy and De Weirdt, Stijn},
  date = {2012-11},
  pages = {572--582},
  doi = {10.1109/SC.Companion.2012.81},
  url = {https://ieeexplore.ieee.org/abstract/document/6495863},
  urldate = {2023-10-03},
  abstract = {Maintaining a collection of software installations for a diverse user base can be a tedious, repetitive, error-prone and time-consuming task. Because most end-user software packages for an HPC environment are not readily available in existing OS package managers, they require significant extra effort from the user support team. Reducing this effort would free up a large amount of time for tackling more urgent tasks. In this work, we present EasyBuild, a software installation framework written in Python that aims to support the various installation procedures used by the vast collection of software packages that are typically installed in an HPC environment - catering to widely different user profiles. It is built on top of existing tools, and provides support for well-established installation procedures. Supporting customised installation procedures requires little effort, and sharing implementations of installation procedures becomes very easy. Installing software packages that are supported can be done by issuing a single command, even if dependencies are not available yet. Hence, it simplifies the task of HPC site support teams, and even allows end-users to keep their software installations consistent and up to date.},
  eventtitle = {2012 {{SC Companion}}: {{High Performance Computing}}, {{Networking Storage}} and {{Analysis}}},
  file = {/home/jon/Zotero/storage/KYS8DCP6/Hoste et al. - 2012 - EasyBuild Building Software with Ease.pdf;/home/jon/Zotero/storage/YPGHB5BZ/6495863.html}
}

@report{kennyKubernetesHPCAdministration2021,
  title = {Kubernetes for {{HPC Administration}}.},
  author = {Kenny, Joseph and Knight, Samuel},
  date = {2021-09-01},
  number = {SAND2021-11507C},
  institution = {{Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Sandia National Laboratories, SNL California}},
  url = {https://www.osti.gov/biblio/1887730},
  urldate = {2023-06-29},
  abstract = {Abstract not provided.},
  langid = {english},
  file = {/home/jon/Zotero/storage/H8BAICLF/Kenny and Knight - 2021 - Kubernetes for HPC Administration..pdf}
}

@inproceedings{mitchellExplorationWorkflowManagement2019,
  title = {Exploration of {{Workflow Management Systems Emerging Features}} from {{Users Perspectives}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Mitchell, Ryan and Pottier, Loїc and Jacobs, Steve and family=Silva, given=Rafael Ferreira, prefix=da, useprefix=false and Rynge, Mats and Vahi, Karan and Deelman, Ewa},
  date = {2019-12},
  pages = {4537--4544},
  doi = {10.1109/BigData47090.2019.9005494},
  abstract = {There has been a recent emergence of new workflow applications focused on data analytics and machine learning. This emergence has precipitated a change in the workflow management landscape, causing the development of new dataoriented workflow management systems (WMSs) in addition to the earlier standard of task-oriented WMSs. In this paper, we summarize three general workflow use-cases and explore the unique requirements of each use-case in order to understand how WMSs from both workflow management models meet the requirements of each workflow use-case from the user’s perspective. We analyze the applicability of the two models by carefully describing each model and by providing an examination of the different variations of WMSs that fall under the task driven model. To illustrate the strengths and weaknesses of each workflow management model, we summarize the key features of four production-ready WMSs: Pegasus, Makeflow, Apache Airflow, and Pachyderm. To deepen our analysis of the four WMSs examined in this paper,we implement three real-world use-cases to highlight the specifications and features of each WMS. We present our final assessment of each WMS after considering the following factors: usability, performance, ease of deployment, and relevance. The purpose of this work is to offer insights from the user’s perspective into the research challenges that WMSs currently face due to the evolving workflow landscape.},
  eventtitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Atmospheric modeling,Big Data,Biological system modeling,Computational modeling,Data analysis,Data-driven.,Machine learning,Scientific workflow,Task analysis,Task-driven,Workflow Management System},
  file = {/home/jon/Zotero/storage/SEQSYHKQ/Mitchell et al. - 2019 - Exploration of Workflow Management Systems Emergin.pdf;/home/jon/Zotero/storage/Y3WVEQNM/9005494.html}
}

@article{novellaContainerbasedBioinformaticsPachyderm2019,
  title = {Container-Based Bioinformatics with {{Pachyderm}}},
  author = {Novella, Jon Ander and Emami Khoonsari, Payam and Herman, Stephanie and Whitenack, Daniel and Capuccini, Marco and Burman, Joachim and Kultima, Kim and Spjuth, Ola},
  editor = {Wren, Jonathan},
  date = {2019-03-01},
  journaltitle = {Bioinformatics},
  volume = {35},
  number = {5},
  pages = {839--846},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/bty699},
  url = {https://academic.oup.com/bioinformatics/article/35/5/839/5068160},
  urldate = {2023-06-29},
  abstract = {Motivation: Computational biologists face many challenges related to data size, and they need to manage complicated analyses often including multiple stages and multiple tools, all of which must be deployed to modern infrastructures. To address these challenges and maintain reproducibility of results, researchers need (i) a reliable way to run processing stages in any computational environment, (ii) a well-defined way to orchestrate those processing stages and (iii) a data management layer that tracks data as it moves through the processing pipeline.},
  langid = {english},
  file = {/home/jon/Zotero/storage/C5LF9253/Novella et al. - 2019 - Container-based bioinformatics with Pachyderm.pdf}
}

@online{pachydermPachyderm,
  type = {Code repository},
  title = {Pachyderm},
  author = {{pachyderm}},
  url = {https://github.com/pachyderm/pachyderm},
  urldate = {2023-10-04},
  organization = {{Github repository}},
  file = {/home/jon/Zotero/storage/W96EDFF5/pachyderm.html}
}

@article{schmidtEvaluationDataScience,
  title = {Evaluation von Data Science Workflow Engines für Kubernetes},
  author = {Schmidt, David},
  langid = {ngerman},
  file = {/home/jon/Zotero/storage/NJMEJZQJ/Schmidt - Evaluation von Data Science Workflow Engines für K.pdf}
}

@article{wildeMethodenspektrumWirtschaftsinformatikUeberblick,
  title = {Methodenspektrum der Wirtschaftsinformatik: Überblick und Portfoliobildung},
  author = {Wilde, Thomas and Hess, Thomas},
  langid = {ngerman},
  file = {/home/jon/Zotero/storage/89XM8EK6/Wilde and Hess - Methodenspektrum der Wirtschaftsinformatik Überbl.pdf}
}

@inproceedings{yooSLURMSimpleLinux2003,
  title = {{{SLURM}}: {{Simple Linux Utility}} for {{Resource Management}}},
  shorttitle = {{{SLURM}}},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {44--60},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/10968987_3},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
  isbn = {978-3-540-39727-4},
  langid = {english},
  keywords = {Exit Status,Lawrence Livermore National Laboratory,Message Authentication Code,Remote Execution,Resource Management System},
  file = {/home/jon/Zotero/storage/CIP9VDIE/Yoo et al. - 2003 - SLURM Simple Linux Utility for Resource Managemen.pdf}
}

@article{youScalingSupportVector2015,
  title = {Scaling {{Support Vector Machines}} on Modern {{HPC}} Platforms},
  author = {You, Yang and Fu, Haohuan and Song, Shuaiwen Leon and Randles, Amanda and Kerbyson, Darren and Marquez, Andres and Yang, Guangwen and Hoisie, Adolfy},
  date = {2015-02-01},
  journaltitle = {Journal of Parallel and Distributed Computing},
  series = {Special {{Issue}} on {{Architecture}} and {{Algorithms}} for {{Irregular Applications}}},
  volume = {76},
  pages = {16--31},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.09.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0743731514001683},
  urldate = {2023-10-03},
  abstract = {Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for~x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4–84×~and 18–47×~speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x~GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns.},
  keywords = {Dynamic modeling,Machine learning models,Multi- \& many-core architectures,Optimization techniques,Performance analysis,Support Vector Machine},
  file = {/home/jon/Zotero/storage/VY6R76FR/You et al. - 2015 - Scaling Support Vector Machines on modern HPC plat.pdf;/home/jon/Zotero/storage/J3798C3J/S0743731514001683.html}
}

@article{zhouContainerOrchestrationHPC2021,
  title = {Container Orchestration on {{HPC}} Systems through {{Kubernetes}}},
  author = {Zhou, Naweiluo and Georgiou, Yiannis and Pospieszny, Marcin and Zhong, Li and Zhou, Huan and Niethammer, Christoph and Pejak, Branislav and Marko, Oskar and Hoppe, Dennis},
  date = {2021-02-22},
  journaltitle = {Journal of Cloud Computing},
  volume = {10},
  number = {1},
  pages = {16},
  issn = {2192-113X},
  doi = {10.1186/s13677-021-00231-z},
  url = {https://doi.org/10.1186/s13677-021-00231-z},
  urldate = {2023-07-02},
  abstract = {Containerisation demonstrates its efficiency in application deployment in Cloud Computing. Containers can encapsulate complex programs with their dependencies in isolated environments making applications more portable, hence are being adopted in High Performance Computing (HPC) clusters. Singularity, initially designed for HPC systems, has become their de facto standard container runtime. Nevertheless, conventional HPC workload managers lack micro-service support and deeply-integrated container management, as opposed to container orchestrators. We introduce a Torque-Operator which serves as a bridge between HPC workload manager (TORQUE) and container orchestrator (Kubernetes). We propose a hybrid architecture that integrates HPC and Cloud clusters seamlessly with little interference to HPC systems where container orchestration is performed on two levels.},
  keywords = {Cloud computing,Container orchestration,HPC workload manager,Kubernetes,Singularity,TORQUE},
  file = {/home/jon/Zotero/storage/UMHY2J4N/Zhou et al. - 2021 - Container orchestration on HPC systems through Kub.pdf;/home/jon/Zotero/storage/EYUHHD39/s13677-021-00231-z.html}
}

@online{zotero-41,
  pubstate = {preprint}
}
